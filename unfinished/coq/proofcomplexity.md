I was thinking today about how an automated theorem prover could search for proofs in an optimal way. The ideas I'm currently exploring are for self-improving theorem provers, along the lines of Hutter Search, Goedel Machines and PowerPlay. However, it occurred to me that we can still do pretty well using a basic Levin Search. First I'll give some background, then explain why I ended up changing my mind.

Automated Theorem Proving

An automated theorem prover is a computer program which is given a theorem, a sequence of symbols, to try and prove. It uses the rules of a formal system, for example Type Theory, the manipulate the symbols. If the system's built-in axioms can be transformed into the given theorem, it is proved. The proof is the series of steps taken to make the transformation. Theorem proving in general is an undecidable problem, so there are always improvements waiting to be found.

Levin Search

Levin Search is the optimal way to invert a function when only given binary yes/no observations. For example, if I may have a function f and a value y and I want to find a value x such that f x = y. I know nothing about f, x or y other than I can spot when f x = y. In general x can't be worked out; I need to guess values for x until I guess correctly. What is the best order to make my guesses?

We can turn this into a question of probilities: what is the most likely answer, given that I know nothing about the problem? Ray Solomonoff proved that in such situations there's a universal probability distribution which gives better predictions than any other: we know that the Universe can be modelled as a huge computation, so anything which happens in the Universe can be modelled as the result of a computer program. We don't know which program, so we consider the results of a randomly generated one. The probability of these results is exactly the universal distribution.

It turns out that this distribution mostly depends on the length of the shortest program for each value, known as the value's Kolmogorov Complexity: those values which can only be generated by a long program are much less probable than those with short programs, since the shorter a program is, the more likely it will appear at the start of a randomly generated program. Unfortunately Kolmogorov Complexity is uncomputable due to the Halting Problem, and since the universal distribution depends on it so heavily, that is also uncomputable.

There is a computable approximation to Kolmogorov Complexity, known as Levin Complexity. This adds the logarithm of a program's runtime on to its length, so longer programs can be less complex than shorter ones if they take exponentially less time. If we make our guesses based on their Levin Complexity then we're performing a Levin Search.

The Levin Search algorithm uses a clever interleaving technique to run many programs concurrently. There are exponentially many more longer programs than short ones, so testing programs in order of length leads to an exponential slowdown. We compensate for this by allocating exponentially fewer resources to longer programs than short. We end up with a search algorithm with a running time proportional to the running time of the program with the lowest Levin Complexity.

Combining the Two

Why not integrate automated theorem proving with Levin Search? programs it is enumerating these programs, we can are, the exponentially morewe consider programs Its cleverness lies in holding off the introduction of new programs is that It groups programs are arranged in a tree, where paths down the tree reflect the programs' code, eg. a binary program 001 could be represented with a binary tree, taking a left branch, a left branch then a right branch. Every time we go down a branch we divide our time slice by the tree's arity (eg. 2 for a binary tree). We start at 100% at the route and when we hit a program (a leaf) we run it for the remaining timeslice.